{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le projet d'OCVX du groupe de Hadrien Berthier, Axel Petit et Franck Thang\n",
    "Ce projet est rélalisé en python 3, et utilisera les bibliothèques scipy numpy et panda\n",
    "Ce document fait objet de rapport, en plus du code il comportera des explications et des annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(f, x, i=0, dx=1e-6):\n",
    "    \"\"\"Computes i-th partial derivative of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which partial derivative is computed.\n",
    "        i: coordinate along which derivative is computed.\n",
    "        dx: slack for finite difference.\n",
    "        \n",
    "    Output:\n",
    "        (float)\n",
    "\n",
    "    \"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    h = np.zeros(x.shape)\n",
    "    h[0, i] = dx\n",
    "    return (f(x + h) - f(x - h)) / (2*dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, x, dx=1e-6):\n",
    "    \"\"\"Computes gradient of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which gradient is computed.\n",
    "        dx: slack for finite difference of partial derivatives.\n",
    "        \n",
    "    Output:\n",
    "        (ndarray) of size domain of f.\n",
    "        \n",
    "    \"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    dim = x.shape[1]\n",
    "    return np.array([partial(f, x, i, dx) for i in range(dim)]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD():\n",
    "    \"\"\"Gradient Descent Object.\n",
    "    \n",
    "    Implements gradient descent aiming to compute optimal objective \n",
    "    value of convex functions and local optimal ones of none \n",
    "    convex functions.\n",
    "    \n",
    "    \"\"\"    \n",
    "    def __init__(self, d_dir=gradient, \n",
    "                 rate=(lambda x, y, z, grad: 0.01), \n",
    "                 decay=(lambda x: np.linalg.norm(x)), \n",
    "                 tol=1e-6, max_iter=1000, grad=gradient):\n",
    "        \"\"\"        \n",
    "        Instantiates a GD object.\n",
    "    \n",
    "        Attributes:\n",
    "        d_dir: function computing descent direction.\n",
    "        rate: function computing learning rate ; takes in\n",
    "              - f (function): objective function\n",
    "              - x (ndarray): current iterate\n",
    "              - dir_x (ndarray): output of a descent direction function\n",
    "              - grad (ndarray): gradient function.\n",
    "        decay: function computing decay.\n",
    "        tol: slack tolerance.\n",
    "        max_iter: upper bound on number of iterations.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.d_dir = d_dir\n",
    "        self.rate = rate\n",
    "        self.decay = decay\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.grad = gradient\n",
    "    \n",
    "    def __call__(self, x, f, rate=None):\n",
    "        \"\"\"Calling gradient descent object with specific starting point and optimal function.\n",
    "        \n",
    "        Args:\n",
    "            x: initial starting point for descent.\n",
    "            f: objective function of optimisation problem.\n",
    "        \n",
    "        Output:\n",
    "            (float) sub-optimal value up to tolerance if execution is proper.\n",
    "            (ndarray) list of gradient descent iterates.\n",
    "            \n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)\n",
    "        n_iter = 0\n",
    "        dir_x = -self.d_dir(f, x, self.tol)\n",
    "        if (rate == None):\n",
    "            delta_x = self.rate(f, x, dir_x, n_iter) * dir_x\n",
    "        else:\n",
    "            delta_x = rate * dir_x\n",
    "        iters, iters_dir = x, delta_x\n",
    "        grad_f_x = self.grad(f, x)\n",
    "        decay = self.decay(grad_f_x)\n",
    "        while decay > self.tol and n_iter < self.max_iter:\n",
    "            ## Deciding on direction\n",
    "            dir_x = -self.d_dir(f, x, self.tol)\n",
    "            if (rate == None):\n",
    "                delta_x = self.rate(f, x, dir_x, n_iter) * dir_x\n",
    "            else:\n",
    "                delta_x = rate * dir_x            \n",
    "            ## Updating iterate\n",
    "            x = x + delta_x\n",
    "            ## Storing on-going data\n",
    "            iters_dir = np.vstack([iters_dir, delta_x])\n",
    "            iters = np.vstack([iters, x])\n",
    "            ## Computing decay\n",
    "            grad_f_x = self.grad(f, x)\n",
    "            decay = self.decay(grad_f_x)\n",
    "            ## Updating iteration number\n",
    "            n_iter += 1\n",
    "        msg = \" Iteration nu. = {}\\n approx. = {}\\n ob value = {}\\n and decay = {}.\"\n",
    "        print(msg.format(n_iter, x.flatten(), f(x), decay))\n",
    "        if decay > self.tol:\n",
    "            warnings.warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)\n",
    "        return (x, iters, iters_dir, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration nu. = 290\n",
      " approx. = [-0.33333317]\n",
      " ob value = [0.66666667]\n",
      " and decay = 9.987566329527908e-07.\n"
     ]
    }
   ],
   "source": [
    "DG_classic = GD()\n",
    "op_pt, iters, iters_dir, n_iter = DG_classic(np.array([10]), lambda x: 3*x[0]**2 + 2*x[0] + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
